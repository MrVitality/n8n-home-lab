# =============================================================================
# DOCKER COMPOSE - Localhost-Only Secure VPS Setup
# =============================================================================
#
# All ports bound to 127.0.0.1 - Access via SSH tunneling
# Uses Dokploy's existing Traefik (no separate Traefik instance)
# Optimized for: 4 vCPU / 16GB RAM / 200GB NVMe
#
# ACCESS METHOD: SSH Port Forwarding
# Example: ssh -L 5678:localhost:5678 -L 3001:localhost:3001 user@your-vps
#
# SERVICE PORTS (localhost only):
#   - n8n:              5678
#   - Supabase Studio:  3001
#   - Supabase API:     8000
#   - Open WebUI:       8082
#   - SearXNG:          8081
#   - Ollama:           11434
#   - PostgreSQL:       5432
#   - Redis:            6379
#   - Node Exporter:    9100
#
# =============================================================================

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "50m"
    max-file: "5"

x-security-opts: &security-opts
  security_opt:
    - no-new-privileges:true

services:
  # ===========================================================================
  # N8N MAIN - Workflow automation UI and webhook handler
  # ===========================================================================
  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:5678:5678"
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - NODE_ENV=production
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-UTC}
      - TZ=${GENERIC_TIMEZONE:-UTC}
      # Database
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=supabase-db
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=postgres
      - DB_POSTGRESDB_USER=${POSTGRES_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_POOL_SIZE=10
      # Queue mode
      - EXECUTIONS_MODE=queue
      - QUEUE_BULL_REDIS_HOST=redis
      - QUEUE_BULL_REDIS_PORT=6379
      - QUEUE_HEALTH_CHECK_ACTIVE=true
      # Security
      - N8N_ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=false
      # Runners
      - N8N_RUNNERS_ENABLED=true
      - N8N_RUNNERS_MODE=external
      - N8N_RUNNERS_AUTH_TOKEN=${N8N_RUNNERS_SECRET}
      # Ollama
      - OLLAMA_HOST=http://ollama:11434
      # Performance
      - N8N_PAYLOAD_SIZE_MAX=64
      - EXECUTIONS_DATA_PRUNE=true
      - EXECUTIONS_DATA_MAX_AGE=336
      - EXECUTIONS_DATA_PRUNE_MAX_COUNT=50000
      - OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS=true
      - NODE_OPTIONS=--max-old-space-size=1024
    volumes:
      - n8n_data:/home/node/.n8n
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:5678/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    depends_on:
      supabase-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - dokploy-network
      - internal
    labels:
      - traefik.enable=false
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 1536M
        reservations:
          memory: 512M

  # ===========================================================================
  # N8N WORKER - Processes queued workflow executions
  # ===========================================================================
  n8n-worker:
    image: n8nio/n8n:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    stop_grace_period: 60s
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - NODE_ENV=production
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-UTC}
      - TZ=${GENERIC_TIMEZONE:-UTC}
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=supabase-db
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=postgres
      - DB_POSTGRESDB_USER=${POSTGRES_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_POOL_SIZE=8
      - EXECUTIONS_MODE=queue
      - QUEUE_BULL_REDIS_HOST=redis
      - QUEUE_BULL_REDIS_PORT=6379
      - QUEUE_HEALTH_CHECK_ACTIVE=true
      - N8N_ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - N8N_RUNNERS_ENABLED=true
      - N8N_RUNNERS_MODE=external
      - N8N_RUNNERS_AUTH_TOKEN=${N8N_RUNNERS_SECRET}
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=false
      - OLLAMA_HOST=http://ollama:11434
      - N8N_CONCURRENCY_PRODUCTION_LIMIT=15
      - NODE_OPTIONS=--max-old-space-size=768
    command: worker
    volumes:
      - n8n_data:/home/node/.n8n
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'n8n worker' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      n8n:
        condition: service_healthy
      redis:
        condition: service_healthy
      supabase-db:
        condition: service_healthy
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1024M
        reservations:
          memory: 384M

  # ===========================================================================
  # N8N RUNNER - Sandboxed code execution for Code nodes
  # ===========================================================================
  n8n-runner:
    image: n8nio/runners:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    environment:
      - N8N_RUNNERS_TASK_BROKER_URI=http://n8n:5679
      - N8N_RUNNERS_AUTH_TOKEN=${N8N_RUNNERS_SECRET}
      - N8N_ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-UTC}
      - N8N_RUNNERS_MAX_CONCURRENCY=20
      - N8N_RUNNERS_TASK_TIMEOUT=180
      - N8N_RUNNERS_HEALTH_CHECK_PORT=5680
      - N8N_RUNNERS_AUTO_SHUTDOWN_TIMEOUT=120
      - NODE_OPTIONS=--max-old-space-size=512
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:5680/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      n8n:
        condition: service_healthy
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 768M
        reservations:
          memory: 256M

  # ===========================================================================
  # OLLAMA - Local LLM inference engine
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:11434:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=10m
      - OLLAMA_MAX_QUEUE=10
    volumes:
      - ollama_storage:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "3.0"
          memory: 5120M
        reservations:
          memory: 2048M

  # ===========================================================================
  # REDIS - Job queue for n8n worker mode
  # ===========================================================================
  redis:
    image: redis:7.4-alpine
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:6379:6379"
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --save ""
      --tcp-keepalive 300
      --timeout 0
    volumes:
      - redis_storage:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 384M
        reservations:
          memory: 128M

  # ===========================================================================
  # SUPABASE DATABASE - PostgreSQL 15
  # ===========================================================================
  supabase-db:
    image: supabase/postgres:15.6.1.145
    hostname: supabase-db
    restart: unless-stopped
    logging: *default-logging
    ports:
      - "127.0.0.1:5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
      JWT_SECRET: ${SUPABASE_JWT_SECRET}
      JWT_EXP: "3600"
    command:
      - postgres
      - -c
      - listen_addresses=*
      - -c
      - shared_buffers=512MB
      - -c
      - effective_cache_size=1536MB
      - -c
      - maintenance_work_mem=128MB
      - -c
      - work_mem=16MB
      - -c
      - wal_buffers=16MB
      - -c
      - max_connections=100
      - -c
      - checkpoint_completion_target=0.9
      - -c
      - default_statistics_target=100
      - -c
      - random_page_cost=1.1
      - -c
      - effective_io_concurrency=200
      - -c
      - min_wal_size=256MB
      - -c
      - max_wal_size=1GB
      - -c
      - max_parallel_workers_per_gather=2
      - -c
      - max_parallel_workers=4
      - -c
      - max_parallel_maintenance_workers=2
      - -c
      - log_min_duration_statement=500
      - -c
      - idle_in_transaction_session_timeout=120000
      - -c
      - statement_timeout=600000
      - -c
      - huge_pages=try
    volumes:
      - supabase_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U ${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2048M
        reservations:
          memory: 768M

  # ===========================================================================
  # SUPABASE POSTGREST - RESTful API for Postgres
  # ===========================================================================
  supabase-rest:
    image: postgrest/postgrest:v12.2.3
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    environment:
      PGRST_DB_URI: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      PGRST_DB_SCHEMAS: public,storage,graphql_public
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${SUPABASE_JWT_SECRET}
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_APP_SETTINGS_JWT_SECRET: ${SUPABASE_JWT_SECRET}
      PGRST_APP_SETTINGS_JWT_EXP: "3600"
      PGRST_DB_POOL: "15"
      PGRST_DB_POOL_ACQUISITION_TIMEOUT: "10"
    depends_on:
      supabase-db:
        condition: service_healthy
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 192M
        reservations:
          memory: 64M

  # ===========================================================================
  # SUPABASE AUTH (GoTrue) - Authentication service
  # ===========================================================================
  supabase-auth:
    image: supabase/gotrue:v2.164.0
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: "9999"
      API_EXTERNAL_URL: http://localhost:8000
      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      GOTRUE_SITE_URL: http://localhost:3001
      GOTRUE_URI_ALLOW_LIST: ""
      GOTRUE_DISABLE_SIGNUP: ${SUPABASE_DISABLE_SIGNUP:-false}
      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_EXP: "3600"
      GOTRUE_JWT_SECRET: ${SUPABASE_JWT_SECRET}
      GOTRUE_EXTERNAL_EMAIL_ENABLED: "true"
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: "false"
      GOTRUE_MAILER_AUTOCONFIRM: ${SUPABASE_MAILER_AUTOCONFIRM:-true}
      GOTRUE_SMTP_HOST: ${SMTP_HOST:-}
      GOTRUE_SMTP_PORT: ${SMTP_PORT:-587}
      GOTRUE_SMTP_USER: ${SMTP_USER:-}
      GOTRUE_SMTP_PASS: ${SMTP_PASS:-}
      GOTRUE_SMTP_ADMIN_EMAIL: ${SMTP_USER:-}
      GOTRUE_SMTP_MAX_FREQUENCY: 1ns
      GOTRUE_MAILER_URLPATHS_INVITE: /auth/v1/verify
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: /auth/v1/verify
      GOTRUE_MAILER_URLPATHS_RECOVERY: /auth/v1/verify
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: /auth/v1/verify
      GOTRUE_RATE_LIMIT_HEADER: X-Forwarded-For
    depends_on:
      supabase-db:
        condition: service_healthy
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 192M
        reservations:
          memory: 64M

  # ===========================================================================
  # SUPABASE META - Postgres metadata API for Studio
  # ===========================================================================
  supabase-meta:
    image: supabase/postgres-meta:v0.84.2
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    environment:
      PG_META_PORT: "8080"
      PG_META_DB_HOST: supabase-db
      PG_META_DB_PORT: "5432"
      PG_META_DB_NAME: postgres
      PG_META_DB_USER: ${POSTGRES_USER}
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}
    depends_on:
      supabase-db:
        condition: service_healthy
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 192M
        reservations:
          memory: 64M

  # ===========================================================================
  # SUPABASE KONG - API Gateway
  # ===========================================================================
  supabase-kong:
    image: kong:2.8.1
    restart: unless-stopped
    ports:
      - "127.0.0.1:8000:8000"
    logging: *default-logging
    user: root
    ulimits:
      nofile:
        soft: 8192
        hard: 65536
    entrypoint:
      - /bin/sh
      - -c
      - |
        cat > /home/kong/kong.yml << 'KONGEOF'
        _format_version: "2.1"
        _transform: true
        consumers:
          - username: anon
            keyauth_credentials:
              - key: ${SUPABASE_ANON_KEY}
          - username: service_role
            keyauth_credentials:
              - key: ${SUPABASE_SERVICE_ROLE_KEY}
        acls:
          - consumer: anon
            group: anon
          - consumer: service_role
            group: admin
        services:
          - name: auth-v1
            url: http://supabase-auth:9999/
            routes:
              - name: auth-v1-all
                strip_path: true
                paths:
                  - /auth/v1/
            plugins:
              - name: cors
              - name: key-auth
                config:
                  hide_credentials: true
              - name: acl
                config:
                  hide_groups_header: true
                  allow:
                    - admin
                    - anon
          - name: rest-v1
            url: http://supabase-rest:8000/
            routes:
              - name: rest-v1-all
                strip_path: true
                paths:
                  - /rest/v1/
            plugins:
              - name: cors
              - name: key-auth
                config:
                  hide_credentials: true
              - name: acl
                config:
                  hide_groups_header: true
                  allow:
                    - admin
                    - anon
          - name: meta
            url: http://supabase-meta:8080/
            routes:
              - name: meta-all
                strip_path: true
                paths:
                  - /pg/
            plugins:
              - name: key-auth
                config:
                  hide_credentials: true
              - name: acl
                config:
                  hide_groups_header: true
                  allow:
                    - admin
        KONGEOF
        chown kong:kong /home/kong/kong.yml
        exec /docker-entrypoint.sh kong docker-start
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_PROXY_LISTEN: 0.0.0.0:8000
      KONG_ADMIN_LISTEN: 127.0.0.1:8001
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 256k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 128 256k
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_ROLE_KEY}
      KONG_MEM_CACHE_SIZE: 128m
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      supabase-db:
        condition: service_healthy
    networks:
      - dokploy-network
      - internal
    labels:
      - traefik.enable=false
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          memory: 192M

  # ===========================================================================
  # SUPABASE STUDIO - Admin dashboard
  # ===========================================================================
  supabase-studio:
    image: supabase/studio:20241202-71e5240
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:3001:3000"
    environment:
      STUDIO_PG_META_URL: http://supabase-meta:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DEFAULT_ORGANIZATION_NAME: Default Organization
      DEFAULT_PROJECT_NAME: Default Project
      SUPABASE_URL: http://localhost:8000
      SUPABASE_PUBLIC_URL: http://localhost:8000
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_ROLE_KEY}
      NEXT_PUBLIC_ENABLE_LOGS: "true"
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
    depends_on:
      supabase-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "true"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - dokploy-network
      - internal
    labels:
      - traefik.enable=false
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 768M
        reservations:
          memory: 256M

  # ===========================================================================
  # OPEN WEBUI - Chat interface for Ollama
  # ===========================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:8082:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_URL=http://localhost:8082
      - WEBUI_SECRET_KEY=${OPENWEBUI_SECRET_KEY}
      - ENABLE_SIGNUP=true
      - ENABLE_LOGIN_FORM=true
      - DEFAULT_MODELS=llama3.2:latest
      - WEBUI_NAME=ManixSystems AI
      - UVICORN_WORKERS=2
      - RAG_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - CHUNK_SIZE=1500
      - CHUNK_OVERLAP=100
    volumes:
      - open_webui_data:/app/backend/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - dokploy-network
      - internal
    labels:
      - traefik.enable=false
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2048M
        reservations:
          memory: 512M

  # ===========================================================================
  # SEARXNG - Privacy-respecting metasearch engine
  # ===========================================================================
  searxng:
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:8081:8080"
    environment:
      - SEARXNG_BASE_URL=http://localhost:8081
      - SEARXNG_SECRET=${SEARXNG_SECRET_KEY}
    command:
      - /bin/sh
      - -c
      - |
        cat > /etc/searxng/settings.yml << 'SETTINGSEOF'
        use_default_settings: true
        general:
          instance_name: "ManixSystems Search"
          privacypolicy_url: false
          donation_url: false
          contact_url: false
          enable_metrics: false
        search:
          safe_search: 0
          autocomplete: "google"
          default_lang: "en"
        server:
          secret_key: "${SEARXNG_SECRET_KEY}"
          limiter: false
          public_instance: false
          image_proxy: true
          method: "POST"
          http_protocol_version: "1.1"
          bind_address: "0.0.0.0:8080"
        ui:
          static_use_hash: true
          default_theme: simple
          theme_args:
            simple_style: dark
        enabled_plugins:
          - 'Hash plugin'
          - 'Self Information'
          - 'Tracker URL remover'
          - 'Ahmia blacklist'
        outgoing:
          request_timeout: 5.0
          max_request_timeout: 15.0
          pool_connections: 200
          pool_maxsize: 20
          keepalive_expiry: 10.0
        SETTINGSEOF
        cat > /etc/searxng/limiter.toml << 'LIMITEREOF'
        [botdetection.ip_limit]
        link_token = false
        [botdetection.ip_lists]
        pass_searxng_org = false
        LIMITEREOF
        exec python -m searx.webapp
    volumes:
      - searxng_data:/var/cache/searxng
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8080/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - dokploy-network
      - internal
    labels:
      - traefik.enable=false
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 384M
        reservations:
          memory: 128M

  # ===========================================================================
  # NODE EXPORTER - Host metrics (Prometheus format)
  # ===========================================================================
  node-exporter:
    image: prom/node-exporter:v1.7.0
    restart: unless-stopped
    <<: *security-opts
    logging: *default-logging
    ports:
      - "127.0.0.1:9100:9100"
    command:
      - '--path.rootfs=/host'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--web.listen-address=:9100'
    volumes:
      - /:/host:ro,rslave
    networks:
      - internal
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 64M

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  n8n_data:
  supabase_db_data:
  ollama_storage:
  redis_storage:
  open_webui_data:
  searxng_data:

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  dokploy-network:
    external: true
  internal:
    driver: bridge
    internal: true  # No external access - containers only
